{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a28934-b360-42fe-bf81-d12fcbffd15e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import IntegerType,StringType,DateType,FloatType,DecimalType\n",
    "from pyspark.sql.window import Window\n",
    "from random import randint,uniform\n",
    "from datetime import datetime\n",
    "import decimal\n",
    "myappdl_sas = dbutils.secrets.get(\"myApp_scope\",\"SasToken\")\n",
    "spark.conf.set(\"fs.azure.account.auth.type.myappdl.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.myappdl.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.myappdl.dfs.core.windows.net\", myappdl_sas)\n",
    "spark.conf.set(\"Spark.jars\",\"dbfs:/FileStore/mssql_jdbc_12_2_0_jre11.jar\")\n",
    "jdbcHostname=dbutils.secrets.get(\"myApp_scope\",\"jdbcHostname\")\n",
    "jdbcDatabase=dbutils.secrets.get(\"myApp_scope\",\"jdbcDatabase\")\n",
    "jdbcPassword=dbutils.secrets.get(\"myApp_scope\",\"jdbcPassword\")\n",
    "jdbcUsername=dbutils.secrets.get(\"myApp_scope\",\"jdbcUsername\")\n",
    "jdbcURL='jdbc:sqlserver://'+jdbcHostname+':1433;database={'+jdbcDatabase+'}'\n",
    "connectionProperties ={\n",
    "    \"user\":jdbcUsername,\n",
    "    \"password\":jdbcPassword,\n",
    "    \"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "@udf(returnType=IntegerType()) \n",
    "def rndm():\n",
    "    return randint(1,1000)\n",
    "@udf(returnType=IntegerType())\n",
    "def indx():\n",
    "    arr=[1,4,6,12]\n",
    "    return arr[randint(0,3)]\n",
    "@udf(returnType=DateType())\n",
    "def NBD():\n",
    "    arr=[str(i) for i in list(range(1,13,1))]\n",
    "    return datetime.strptime('01/'+arr[randint(0,11)]+'/2023',\"%d/%m/%Y\")\n",
    "@udf(returnType=StringType())\n",
    "def randomNum():\n",
    "    return ['Active','Ended'][randint(0,1)]\n",
    "@udf(returnType=DecimalType(20,2))\n",
    "def rndmDecimal():\n",
    "    return decimal.Decimal(uniform(1,1000))\n",
    "\n",
    "def createContract():\n",
    "    contracts = spark.read.jdbc(url=jdbcURL,table='Contracts',properties=connectionProperties)\n",
    "    company = spark.read.jdbc(url=jdbcURL,table='Company',properties=connectionProperties)\n",
    "    contracts.createOrReplaceTempView('Conts')\n",
    "    company.createOrReplaceTempView('company')\n",
    "    maxid=contracts.agg({\"id\": \"max\"}).collect()[0][\"max(id)\"]\n",
    "    if maxid==None:\n",
    "        maxid=0\n",
    "    contractCntBYyNSO = spark.sql(\"Select distinct c.companyid companyid,count(*) mx from company n left join conts c group by c.companyid\")\n",
    "    if(len(contracts.head(1))==0):\n",
    "        contractCntBYyNSO = spark.sql(\"Select distinct id companyid,0 mx from company\")\n",
    "    \n",
    "    contracts1=users.alias('u').join(f.broadcast(contractCntBYyNSO).alias('cn'),f.col('u.companyid')==f.col('cn.companyid'),'inner')\\\n",
    "        .join(f.broadcast(company).alias('c'),f.col('u.companyid')==f.col('c.id'),'inner')\\\n",
    "        .join(user_data.alias('ud'),f.col('u.id')==f.col('ud.userid'),'inner')\\\n",
    "        .select((maxid+f.row_number().over(Window.orderBy(f.col('u.id').asc()))).alias('id'),\\\n",
    "        f.concat(f.substring(f.col('c.country'),0,4),f.lit('-'),(f.row_number().over(Window.partitionBy(f.col('u.companyid')).orderBy(f.col('u.id').asc()))+f.col('cn.mx')).cast(StringType())).alias('contractNumber'),\\\n",
    "        f.col('u.id').alias('userid'),\\\n",
    "        f.col('u.startdate').cast(DateType()).alias('startdate'),\\\n",
    "        f.col('ud.id').alias('invoiceaddressid'),\\\n",
    "        f.col('u.companyid').alias('companyid'))\n",
    "    contracts1=contracts1.withColumn(\"productid\",rndm()).withColumn(\"billingFreq\",indx()).withColumn(\"nextBillingDate\",NBD())\\\n",
    "    .withColumn('BillingStatus',f.lit('Active')).withColumn('ContractStatus',randomNum()).withColumn('serviceCharge',rndmDecimal())\n",
    "    if(len(contracts1.head(1))!=0):\n",
    "        display(contracts1)\n",
    "        dirname = datetime.today().strftime('%Y-%m-%d')\n",
    "        contracts1.write.csv('abfss://raw@myappdl.dfs.core.windows.net/Contracts/Contract_'+dirname+'/',mode='overwrite',header=True)\n",
    "        contracts1.write.jdbc(url=jdbcURL,table='Contracts',mode='append',properties=connectionProperties)\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "\n",
    "rawData=spark.read.csv('abfss://raw@myappdl.dfs.core.windows.net/API_data/UserData*.csv',header=True)\n",
    "nso = spark.read.jdbc(url=jdbcURL,table='Company',properties=connectionProperties)\n",
    "nso.createOrReplaceTempView('NSO')\n",
    "rawData.createOrReplaceTempView('temp_table')\n",
    "\n",
    "users = spark.read.jdbc(url=jdbcURL,table='Users',properties=connectionProperties)\n",
    "users.createOrReplaceTempView('users')\n",
    "maxid = users.agg({\"id\": \"max\"}).collect()[0][\"max(id)\"]\n",
    "if maxid==None:\n",
    "    maxid= 0\n",
    "users = spark.sql(f\"select distinct {maxid}+row_number() over (order by t.registration_date asc) id,t.title||' '||t.firstname||' '||t.lastname name,\\\n",
    "    t.registration_date StartDate, t.uuid uniqueidentifier, t.dob_date dob,t.gender gender,t.username username, t.password password, n.id companyid \\\n",
    "        from temp_table t inner join NSO n on n.country=upper(t.country) \\\n",
    "            where not exists (select 1 from users u where u.uniqueidentifier=t.uuid)\")\n",
    "users.createOrReplaceTempView('users')\n",
    "display(users)\n",
    "user_data= spark.sql(f\"select distinct {maxid}+row_number() over (partition by n.id order by t.uuid asc) id, u.id userid, \\\n",
    "    t.title||' '||t.firstname||' '||t.lastname||', '||t.streetName||'-'||t.streetnumber||', '||t.city||', '||t.state||', '||t.country||'- '||t.postcode invoiceaddress, \\\n",
    "        n.id companyid, t.latitude lat, t.longitude lng, t.email emailid, t.phone phone, t.cell mobno\\\n",
    "            from temp_table t inner join NSO n on n.country=upper(t.country)\\\n",
    "                inner join users u on u.uniqueidentifier=t.uuid\")\n",
    "user_data.createOrReplaceTempView(\"userdata\")\n",
    "display(user_data)\n",
    "\n",
    "if(len(users.head(1))!=0 & createContract()):\n",
    "    dirname = datetime.today().strftime('%Y-%m-%d')\n",
    "    users.write.csv('abfss://raw@myappdl.dfs.core.windows.net/User/Users_'+dirname+'/',mode='overwrite',header=True)\n",
    "    user_data.write.csv('abfss://raw@myappdl.dfs.core.windows.net/User/UserData_'+dirname+'/',mode='overwrite',header=True)\n",
    "    users.write.jdbc(url=jdbcURL,table='Users',mode='append',properties=connectionProperties)\n",
    "    user_data.write.jdbc(url=jdbcURL,table='UserData',mode='append',properties=connectionProperties)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1351544504933439,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "createUsersAndContracts",
   "widgets": {
    "nononi": {
     "currentValue": " 1",
     "nuid": "3acaf9d9-fea6-4903-9521-56a5a62c2e1c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": " ",
      "label": null,
      "name": "nononi",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
