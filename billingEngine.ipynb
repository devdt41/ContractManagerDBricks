{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efc0956-2cea-4f33-a97c-50fb5508cd2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install JayDeBeApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b78248-021e-4c5f-ab22-a17832fe9b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType,DateType,ArrayType,StringType\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json\n",
    "import jaydebeapi\n",
    "import uuid\n",
    "@udf (returnType=ArrayType(DateType()))\n",
    "def getDates(strDate,freq,bno):\n",
    "    dates=[strDate]\n",
    "    bno-=1\n",
    "    for i in range(0,bno):\n",
    "        date=dates[i] + relativedelta(months=freq)\n",
    "        dates.append(date)\n",
    "    return dates\n",
    "@udf (returnType=StringType())\n",
    "def uuidGen():\n",
    "    return str(uuid.uuid4())\n",
    "@udf (returnType=DateType())\n",
    "def nbdGen(n1,freq):\n",
    "    return n1+ relativedelta(months=freq)\n",
    "\n",
    "def updateNBD(contractid,companyid,nbd):\n",
    "    cursor = connection.cursor()\n",
    "    nbd=datetime.strptime(nbd,\"%d/%m/%Y\")\n",
    "    query=f\"Update [dbo].[contracts] set nextBillingDate='{nbd}' where id={contractid}  and companyid={companyid}\"\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "\n",
    "def createInvoices(billableItems1):\n",
    "    invoices=spark.read.jdbc(url=jdbcURL,table='invoice',properties=connectionProperties)\n",
    "    maxid=invoices.agg({'id':'max'}).collect()[0]['max(id)']\n",
    "    if(maxid==None):\n",
    "        maxid=0\n",
    "    billableItems1=billableItems1.withColumn('id',f.col('id')+maxid).drop('Billings')\n",
    "    display(billableItems1)\n",
    "    billableItems1.printSchema()\n",
    "    dirname = datetime.today().strftime('%Y-%m-%d')\n",
    "    billableItems1.write.csv('abfss://raw@myappdl.dfs.core.windows.net/Contracts/Invoices_'+dirname+'/',mode='overwrite',header=True)\n",
    "    billableItems1.write.jdbc(url=jdbcURL,mode='append',table='invoice',properties=connectionProperties)\n",
    "\n",
    "def SelectionCriteria():\n",
    "    lst=[]\n",
    "    lstFive=[]\n",
    "    lastNso=None\n",
    "    results=spark.sql(\"select id,companyId companyId,nextBillingDate,billingFreq,DATEDIFF(MONTH, nextBillingDate, CURRENT_TIMESTAMP) Months,invoiceaddressid,serviceCharge from contracts where nextBillingDate<=CURRENT_TIMESTAMP and upper(billingstatus)='ACTIVE'\\\n",
    "        group by id,companyId,billingFreq,nextBillingDate,serviceCharge,invoiceaddressid order by companyId asc\")\n",
    "    \n",
    "    results=results.withColumn('Billings',f.floor(f.col('Months')/f.col('billingFreq')).cast(IntegerType())+1)\\\n",
    "    .withColumn(\"invoiceDates\",f.array(getDates(f.col('nextBillingDate'),f.col('billingFreq'),f.col('Billings')))[0])\n",
    "    display(results)\n",
    "\n",
    "    billableItems=results.withColumn('contractId',f.col('id')).withColumn('invoiceDate',f.explode(f.col('invoiceDates'))).\\\n",
    "        withColumn('id',f.row_number().over(Window.orderBy(f.col('contractId').asc()))).withColumn('uuid',uuidGen()).\\\n",
    "        withColumn('finalInvoiceNumber',f.lit(None).cast(IntegerType())).\\\n",
    "        select('id','finalInvoiceNumber','uuid','contractId','companyid','Billings','invoiceDate','invoiceaddressid',f.col('serviceCharge').alias('Revenue'))\n",
    "        \n",
    "    billableItems.printSchema()\n",
    "\n",
    "    resultsByNSO = results.withColumn('nbd',nbdGen(f.col('invoiceDates')[f.size(f.col('invoiceDates'))-1],f.col('billingFreq'))).select('id','companyId','nbd').collect()\n",
    "    createInvoices(billableItems)\n",
    "    try:\n",
    "        for i in resultsByNSO:\n",
    "            contractid=i['id']\n",
    "            companyid=i['companyId']\n",
    "            nbd=i['nbd'].strftime(\"%d/%m/%Y\")\n",
    "            updateNBD(contractid,companyid,nbd)\n",
    "    except :\n",
    "        connection.rollback()\n",
    "        raise ValueError('something went wrong')\n",
    "\n",
    "\n",
    "myappdl_sas = dbutils.secrets.get(\"myApp_scope\",\"SasToken\")\n",
    "spark.conf.set(\"fs.azure.account.auth.type.myappdl.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.myappdl.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.myappdl.dfs.core.windows.net\", myappdl_sas)\n",
    "spark.conf.set(\"Spark.jars\",\"dbfs:/FileStore/mssql_jdbc_12_2_0_jre11.jar\")\n",
    "jdbcHostname=dbutils.secrets.get(\"myApp_scope\",\"jdbcHostname\")\n",
    "jdbcDatabase=dbutils.secrets.get(\"myApp_scope\",\"jdbcDatabase\")\n",
    "jdbcPassword=dbutils.secrets.get(\"myApp_scope\",\"jdbcPassword\")\n",
    "jdbcUsername=dbutils.secrets.get(\"myApp_scope\",\"jdbcUsername\")\n",
    "jdbcURL='jdbc:sqlserver://'+jdbcHostname+':1433;database={'+jdbcDatabase+'}'\n",
    "connectionProperties ={\n",
    "    \"user\":jdbcUsername,\n",
    "    \"password\":jdbcPassword,\n",
    "    \"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "connection = jaydebeapi.connect(\n",
    "    'com.microsoft.sqlserver.jdbc.SQLServerDriver',\n",
    "    jdbcURL,\n",
    "    {'user': jdbcUsername, 'password': jdbcPassword},  # Connection properties\n",
    "    jars='dbfs:/FileStore/mssql_jdbc_12_2_0_jre11.jar'    # Path to the JDBC driver JAR file\n",
    ")\n",
    "connection.jconn.setAutoCommit(True)\n",
    "contracts=spark.read.jdbc(url=jdbcURL,table='Contracts',properties=connectionProperties)\n",
    "contracts.createOrReplaceTempView(\"contracts\")\n",
    "SelectionCriteria()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "billingEngine",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
